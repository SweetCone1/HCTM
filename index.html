<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Text-guided High-definition Consistency Texture Model</title>
<!--   <script src="https://threejs.org/build/three.js"></script>
  <script src="https://raw.githubusercontent.com/SweetCone1/HCTM/main/OBJLoader.js"></script>
  <script src="https://raw.githubusercontent.com/SweetCone1/HCTM/main/MTLLoader.js"></script> -->
</head>
<body>
  <h1>Text-guided High-definition Consistency Texture Model</h1>
  <h2>Authors: Zhibin Tang, Tiantong He</h2>
  <h3>Abstract</h3>
  <p>
    With the advent of depth-to-image diffusion models, text-guided generation, editing, and transfer of realistic textures are no longer difficult. However, due to the limitations of pre-trained diffusion models, they can only create low-resolution, inconsistent textures. To address this issue, we present the High-definition Consistency Texture Model (HCTM), a novel method that can generate high-definition and consistent textures for 3D meshes according to the text prompts. We achieve this by leveraging a pre-trained depth-to-image diffusion model to generate single viewpoint results based on the text prompt and a depth map. We fine-tune the diffusion model with Parameter-Efficient Fine-Tuning to quickly learn the style of the generated result, and apply the multi-diffusion strategy to produce high-resolution and consistent results from different viewpoints. Furthermore, we propose a strategy that prevents the appearance of noise on the textures caused by backpropagation. Our proposed approach has demonstrated promising results in generating high-definition and consistent textures for 3D meshes, as demonstrated through a series of experiments.
  </p>
  <img src="https://raw.githubusercontent.com/SweetCone1/HCTM/main/pic1.png" alt="pic">
  <p>this is pic1</p>
  <video src="https://raw.githubusercontent.com/SweetCone1/HCTM/main/video1.mp4" controls></video>
  <p>this is video1</p>
  <a href="https://arxiv.org/abs/2305.05901" target="_blank">Link to the arXiv paper</a>
<!--   <div id="3d-model" style="width: 800px; height: 600px;"></div> -->

<!--   <script>
    var scene = new THREE.Scene();
    var camera = new THREE.PerspectiveCamera(75, 800 / 600, 0.1, 1000);
    var renderer = new THREE.WebGLRenderer();

    renderer.setSize(800, 600);
    document.getElementById('3d-model').appendChild(renderer.domElement);

    var mtlLoader = new THREE.MTLLoader();
    mtlLoader.load('https://raw.githubusercontent.com/SweetCone1/HCTM/main/mesh.mtl', function (materials) {
      materials.preload();

      var objLoader = new THREE.OBJLoader();
      objLoader.setMaterials(materials);
      objLoader.load('https://raw.githubusercontent.com/SweetCone1/HCTM/main/mesh.obj', function (object) {
        scene.add(object);
      });
    });

    camera.position.z = 5;

    function animate() {
      requestAnimationFrame(animate);
      renderer.render(scene, camera);
    }

    animate();
  </script> -->
</body>
</html>
